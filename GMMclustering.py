import sys
import logging
import numpy as np
from scipy.misc import logsumexp
from pyspark.mllib.clustering import KMeans


class GMMclustering:

    logging.basicConfig(level=logging.INFO,
                        format='%(levelname)s %(message)s')

    def fit(self, X, k, n_iter):
        """
        Estimate model parameters with the expectation-maximization
        algorithm.

        Parameters
        ----------
        X - RDD of data points
        k - Number of components
        n_iter - Number of iterations. Default to 100

        Attributes
        ----------

        covariance_type : Type of covariance matrix.
            Supports only diagonal covariance matrix.

        convergence_threshold : Threshold value to check the convergence criteria.
            Defaults to 1e-2

        min_covar : Floor on the diagonal of the covariance matrix to prevent
            overfitting.  Defaults to 1e-3.

        converged : True once converged False otherwise.

        Weights : array of shape (1,  k)
            weights for each mixture component.

        Means : array of shape (k, n_dim)
            Mean parameters for each mixture component.

        Covars : array of shape (k, n_dim)
            Covariance parameters for each mixture component

        """
        self.covariance_type = 'diag'
        self.convergence_threshold = 1e-2
        self.min_covar = 1e-3
        self.converged = False
        
         #  observation statistics
        self.s0 = 0
        self.s1 = 0
        
        num_points = X.count()
        if (num_points == 0):
            raise ValueError(
                'Dataset cannot be empty')
        if (num_points < k):
            raise ValueError(
                'Not possible to make (%s) components from (%s) datapoints' %
                (k,  num_points))
        #  To get the no of dimensions
        n_dim = X.first().shape[0]

        # Initialize Means using MLlib KMeans
        self.Means = np.array(KMeans().train(X, k).clusterCenters)

        # Initialize Weights with the value 1/k for each component
        self.Weights = np.tile(1.0 / k, k)

        # Initialize Covars(diagonal covariance matrix)
        cov = []
        for i in range(n_dim):
            cov.append(X.map(lambda m: m[i]).variance()+self.min_covar)
        self.Covars = np.tile(cov,  (k,  1))

        self.InitilaizeParams(n_dim, k)

        #  EM algorithm
        log_likelihood = []

        # loop until number of iterations  or convergence criteria is satisfied
        for i in range(n_iter):
            logging.info("GMM running iteration %s " % i)
            # Expectation Step
            EstepOut = X.map(lambda x: (self.scoreOnePoint(x)))

            # Maximization step
            MstepIn = EstepOut.reduce(lambda (w1, x1, y1, z1), (w2, x2, y2, z2):
                                      (w1+w2, x1+x2,  y1+y2,  z1+z2))
            self.s0 = self.s1
            self.mstepOnePoint(MstepIn[0], MstepIn[1], MstepIn[2], MstepIn[3])

            #  Check for convergence.
            if i > 0 and abs(self.s1-self.s0) < self.convergence_threshold:
                converged = True
                logging.info("Converged at iteration %s" % i)
                break

            self.InitilaizeParams(n_dim, k)

        # self.resposibility_matrix = np.ones([num_points,n_dim])
        # self.cluster_labels = np.ones([num_points])
        # X.map(lambda m: self.predict(m)).collect()
        return self

    def scoreOnePoint(self, x):

        """
        Compute the log likelihood of 'x' being generated under the current model
        Also returns the probability that 'x' is generated by each component of the mixture

        Parameters
        ----------
        x : array of shape (1,  n_dim)
            Corresponds to a single data point.

        Returns
        -------
        log_likelihood_x :Log likelihood  of 'x'
        prob_x : Resposibility  of each cluster for the data point 'x'

        """

        lpr = (self.log_multivariate_normal_density_diag_Nd(x, self.Means, self.Covars) +
               np.log(self.Weights))
        log_likelihood_x = logsumexp(lpr)
        prob_x = np.exp(lpr-log_likelihood_x)
        temp_wt = np.dot(prob_x.T[:, np.newaxis],  x[np.newaxis, :])
        temp_avg = np.dot(prob_x.T[:, np.newaxis], (x*x)[np.newaxis, :])
        return log_likelihood_x, prob_x, temp_wt, temp_avg

    def log_multivariate_normal_density_diag_Nd(self, x, means, covars):
        """
        Compute Gaussian log-density at x for a diagonal model

        """

        n_features = x.shape[0]
        lpr = -0.5 * (n_features*np.log(2*np.pi) + np.sum(np.log(covars), 1) +
                      np.sum((means ** 2) / covars, 1) - 2 * np.dot(x, (means/covars).T) +
                      np.dot(x**2, (1/covars).T))
        return lpr

    def mstepOnePoint(self, log_sum, prob_sum, wt_x_sum, avg_x2_sum):
        """
            Perform the Mstep of the EM algorithm.
            Updates Means, Covars and Weights using observation statistics
        """
        self.s1 = log_sum
        weights_x = prob_sum
        weighted_X_sum = wt_x_sum
        avg_X2 = avg_x2_sum
        inverse_weights_x = 1.0 / (weights_x)
        self.Weights = (weights_x / (weights_x.sum()))
        self.Means = (weighted_X_sum * inverse_weights_x.T[:, np.newaxis])
        self.Covars = ((avg_X2 * inverse_weights_x.T[:, np.newaxis]) - (self.Means**2)
                       + self.min_covar)

    def InitilaizeParams(self, n_dim, k):
        """
            Initilaize some observation statistics
        """
        weights_x = np.zeros(k)
        weighted_X_sum = np.zeros((k, n_dim))
        avg_X2 = np.zeros((k, n_dim))

    def predict(self, x):
        """
            Predicts the cluster to which the given instance belongs to
            based on the maximum resposibility.

        Parameters
        ----------
        x : array of shape (1,  n_dim)
            Corresponds to a single data point.

        Returns
        -------
        p: 
        """
        lpr = (self.log_multivariate_normal_density_diag_Nd(x, self.Means, self.Covars) +
               np.log(self.Weights))
        log_likelihood_x = logsumexp(lpr)
        prob_x = np.exp(lpr-log_likelihood_x)
        resposibility_matrix = np.array(prob_x)
        return resposibility_matrix